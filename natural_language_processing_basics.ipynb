{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb9d6da4-6500-4924-83a2-e5887eb3a09d",
   "metadata": {},
   "source": [
    "# Natural Language Processing\n",
    "Natural Language Processing (NLP) is the subfield of computer science which studies the human languages. As an example in NLP, people explore documents to extract meaningful information and try to train the computers to run algorithms to find the information. Examples of NLP are:\n",
    "-   Text processing\n",
    "-   Speech recognition\n",
    "-   Speech synthesis\n",
    "\n",
    "I would like to work on the **Text processing** part here. Therefore, in this notebook there is a bit of touch in this field of NLP. The fundamental parts of NLP in text processing are: To find stop words, tokenization, stemming, speech tagging.\n",
    "\n",
    "## Stop words\n",
    "There are common words that most of the time do not carry useful information. These words can be removed from the text. These words can be defined by the user which is based on the goal of the information extraction from the text. These words are called **stop words**. In python libraries we can make the use of well-known packages such as `spaCy`, and `NLTK`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55753358-e439-44d9-b326-7815c77c9b35",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the list of stop words in NLTK package:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['been',\n",
       " 'over',\n",
       " 'she',\n",
       " 'hers',\n",
       " 'an',\n",
       " 'herself',\n",
       " \"haven't\",\n",
       " 'now',\n",
       " 'there',\n",
       " 'be']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# stop words in NLTK\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "print(\"Here is the list of stop words in NLTK package:\")\n",
    "list(set(stopwords.words('english')))[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "604ffce9-3ce8-4f88-be71-ac08169e67fd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326\n",
      "{'thereafter', 'hers', 'now', 'there', 'be', 'besides', 'more', '’m', 'toward', 'move', 'but', 'within', 'towards', 'every', 'so', 'therefore', 'through', 'such', 'much', 'made', 'these', 'her', 'keep', 'also', 'were', 'itself', 'various', 'nothing', 'until', 'eight', 'with', 'always', 'hence', 'onto', '‘s', 'never', 'six', 'when', 'along', 'each', 'together', 'and', 'or', 'all', 'very', 'you', 'can', 'ten', 'i', \"'re\", '’re', 'none', 'less', 'no', 'has', 'amount', 'first', 'ourselves', 'too', 're', 'its', 'as', 'some', 'either', 'how', 'ever', 'call', 'twelve', 'everything', 'other', 'nor', 'just', 'thus', 'will', 'well', 'whither', 'sometimes', 'then', 'seems', 'often', 'why', 'moreover', 'been', 'anyhow', 'part', 'perhaps', 'back', 'had', 'among', 'may', 'my', 'once', 'thereupon', 'could', 'whoever', 'on', 'after', 'one', 'sixty', 'where', 'sometime', 'is', 'me', 'any', \"'m\", '’ve', 'five', 'would', 'except', 'yours', '‘d', 'anywhere', 'several', 'indeed', 'somehow', 'something', 'nobody', 'wherever', 'however', 'last', 'than', 'we', 'via', '‘m', 'himself', 'beyond', 'whereas', 'am', 'rather', '’d', \"'d\", 'seem', 'doing', 'see', 'say', 'amongst', 'take', 'themselves', 'between', 'everyone', 'us', 'in', 'due', 'their', 'namely', 'therein', 'out', 'thru', 'bottom', 'whatever', 'have', 'hereby', 'they', 'whole', 'third', 'few', 'it', 'per', 'enough', '‘ve', 'became', \"'ve\", 'down', 'yet', 'everywhere', 'even', 'over', 'she', '‘re', 'alone', 'although', '’s', 'anyone', 'front', 'ca', 'without', 'otherwise', 'really', 'because', 'somewhere', 'off', 'myself', 'here', 'from', 'show', 'both', 'whenever', 'become', 'least', 'up', 'since', 'serious', 'to', 'what', 'whom', 'under', 'meanwhile', 'someone', 'behind', 'beforehand', 'still', 'did', 'of', 'n’t', 'two', 'before', 'please', 'if', 'nowhere', 'wherein', 'unless', 'using', 'another', 'four', 'put', 'the', 'most', 'whence', 'for', 'whereby', 'must', 'seemed', 'whereupon', 'was', 'further', 'not', \"'ll\", 'anything', 'regarding', 'many', 'those', 'own', 'side', 'thereby', 'are', 'give', 'cannot', 'an', 'herself', 'only', 'full', 'others', 'though', 'being', 'n‘t', 'during', '’ll', 'your', 'below', 'does', 'might', 'do', 'into', 'beside', 'latter', 'hereupon', 'he', 'elsewhere', 'whether', 'at', 'anyway', 'becoming', 'which', 'forty', 'fifty', 'already', 'whose', 'three', '‘ll', 'around', 'go', 'neither', 'again', 'above', 'noone', 'who', 'done', 'next', 'thence', 'get', 'becomes', 'should', 'by', 'nevertheless', 'whereafter', \"n't\", 'used', 'upon', 'against', 'else', 'quite', 'while', 'him', 'this', 'former', 'nine', 'top', \"'s\", 'same', 'make', 'about', 'herein', 'name', 'that', 'latterly', 'mine', 'formerly', 'yourselves', 'a', 'afterwards', 'hundred', 'seeming', 'our', 'ours', 'empty', 'fifteen', 'throughout', 'twenty', 'mostly', 'them', 'eleven', 'his', 'yourself', 'hereafter', 'almost', 'across'}\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "#loading the english language small model of spacy\n",
    "en = spacy.load('en_core_web_sm')\n",
    "stopwords = en.Defaults.stop_words\n",
    "\n",
    "print(len(stopwords))\n",
    "print(stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1e0349-e7f1-49c5-ac32-2501e004aa95",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "Splitting the text into sentences and words is called tokenization. In this process we can have different numbers of words bind together. If the sentences are split into n words in a batch is called n-gram. As an example if each two words of sentences are wrapped together is called 2-gram.  \n",
    "\n",
    "To find out more information about tokenization process simply click on [spaCy](https://spacy.io/usage/spacy-101) and [NLTK](https://www.nltk.org/api/nltk.tokenize.html) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7dd1d189-0771-425f-be5a-2b6f2df9fbaa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This\n",
      "sentence\n",
      "will\n",
      "be\n",
      "tokenized\n",
      "here\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"This sentence will be tokenized here.\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5154cb97-f24e-447b-876e-9002ac74f454",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This', 'sentence', 'will', 'be', 'tokenized', 'here', '.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenizing with NLTK\n",
    "from nltk.tokenize import word_tokenize\n",
    "text = [\"This sentence will be tokenized here.\"]\n",
    "word_tokenize(text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b17df836-2097-4424-8c54-aa2b2564f530",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2-grams:\n",
      " ['be tokenized' 'sentence will' 'this sentence' 'tokenized here' 'will be']\n"
     ]
    }
   ],
   "source": [
    "# Tokenizing with countvectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "counter = CountVectorizer(max_features=10, \n",
    "                             ngram_range=(2, 2))\n",
    "\n",
    "counter.fit_transform(text)\n",
    "\n",
    "print(\"2-grams:\\n\",counter.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9b897b-65d0-4114-a9ee-2db96eab8834",
   "metadata": {},
   "source": [
    "## Stemming & Lemmitization\n",
    "Many words have the same root such as: buy, buys, buying, bought. The process of replaccing the words by their root is called **stemming**. \n",
    "**Lemmatization** refers to the process of grouping the words which are from the same root together and all will be analyzed by a single word. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ccf69413-f6ef-49e6-aa55-01bdadf1b62d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This this\n",
      "sentence sentence\n",
      "needs(needed needs(neede\n",
      ", ,\n",
      "need need\n",
      ") )\n",
      "to to\n",
      "be be\n",
      "lemmitized lemmitize\n",
      ". .\n"
     ]
    }
   ],
   "source": [
    "# Spacy\n",
    "\n",
    "doc = nlp('This sentence needs(needed, need) to be lemmitized.')\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2e22878f-22ae-4a39-939e-3aea9a69fbbe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemming for walk is walk\n",
      "Stemming for , is ,\n",
      "Stemming for walking is walk\n",
      "Stemming for , is ,\n",
      "Stemming for walks is walk\n",
      "Stemming for , is ,\n",
      "Stemming for walked is walk\n",
      "Stemming for , is ,\n",
      "Stemming for went is went\n",
      "Stemming for , is ,\n",
      "Stemming for go is go\n",
      "Stemming for , is ,\n",
      "Stemming for goes is goe\n"
     ]
    }
   ],
   "source": [
    "# nltk\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "text =\"walk, walking, walks, walked, went, go, goes\"\n",
    "\n",
    "tokenization = nltk.word_tokenize(text)\n",
    "for w in tokenization:\n",
    "    print(\"Stemming for {} is {}\".format(w,stemmer.stem(w))) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
